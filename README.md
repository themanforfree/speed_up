| Language |  Library | Mean(ms) | P99(ms) | P999(ms) | Max(ms) | LMean(ms) | Bench(ms) |
| -------: | -------: | -------: | ------: | -------: | ------: | --------: | --------: |
|      C++ |    beast |     0.18 |    0.45 |     2.56 |    2.59 |     17.93 |     18.46 |
|     Rust |  reqwest |     0.17 |    0.33 |     1.57 |    1.57 |     16.65 |     23.63 |
|   Python | requests |     3.28 |   10.04 |    14.55 |   15.34 |    328.27 |    336.20 |
|   Python |    beast |     0.06 |    0.09 |     5.87 |   11.48 |      5.64 |     39.68 |
|   Python |  aiohttp |     0.47 |    0.63 |     1.36 |    1.48 |     47.45 |     48.34 |
|   Python |  reqwest |     0.38 |    1.06 |     2.15 |    2.44 |     37.94 |     40.20 |

> PS: Mean/P99/P999/Max 表示的都是单个请求的耗时，LMean 表示的是 10 个线程/任务的平均耗时，Bench 表示每组测试 10 个线程/任务的总耗时。  

整体符合预期，原生 C++ 和 Rust 性能最优，原生 Python 的性能较差。通过 Python 调用 C++/Rust 的库，也能获得更好的性能。
但是有一组数据看起来不太正常，Python 调用 beast 库，反而比 C++ 调用还要更快。
添加了 LMean 和 Bench 两列数据的统计后，观察到 Python 调用 beast 时，总时间消耗远大于单个线程的时间消耗，10 个线程几乎是完全串行执行的，由于 GIL 的存在，CPython 本身就是无法直接通过线程实现真正的并行的，这里的完全串行指的是线程之间没有在 io 任务时发生切换，几乎是一个线程完全执行完才会开始执行一个任务，因此虽然请求的平均耗时较短，但是 1000 个请求的整体耗时却比较长，我又单独测试了在 C++ 中纯串行执行请求，得出的平均耗时也接近 0.06，验证了这个猜想。

查阅资料并修改 C++ 库代码，允许 C++ 在处理 IO 时释放 GIL，再次测试后，结果如下：

| Language |  Library | Mean(ms) | P99(ms) | P999(ms) | Max(ms) | LMean(ms) | Bench(ms) |
| -------: | -------: | -------: | ------: | -------: | ------: | --------: | --------: |
|      C++ |    beast |     0.16 |    0.27 |     1.45 |    1.48 |     15.77 |     16.23 |
|     Rust |  reqwest |     0.14 |    0.37 |     0.62 |    0.64 |     13.88 |     18.01 |
|   Python | requests |     3.07 |    6.12 |     7.04 |    7.95 |    307.41 |    314.91 |
|   Python |    beast |     0.15 |    0.28 |     0.40 |    0.53 |     15.54 |     16.36 |
|   Python |  aiohttp |     0.47 |    0.78 |     1.37 |    1.50 |     47.50 |     48.49 |
|   Python |  reqwest |     0.38 |    0.90 |     1.87 |    2.13 |     38.35 |     40.42 |

可以看到 Python 调用 beast 库的性能已经和 C++ 调用基本一致了，总耗时也降低了很多，基本和单个线程的耗时持平，符合预期。